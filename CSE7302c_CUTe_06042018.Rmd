---
title: "CUTe_03JUN2018 - Income Classifier"
author: "Rohit"
date: "2/Jun/2018"
output: html_notebook
---

Remove/Clear all the environment variables
```{r}

rm(list = ls(all=TRUE))
library(caret)
library(DMwR)
library(glmnet)
library(ROCR)
```
# Data Exploration
## Read the Data and review head & tail
```{r}

#Train Data
train_Income_data <- read.csv("train_data.csv", header = T, sep = ",")
head(train_Income_data)
tail(train_Income_data)

#Test Data
test_Income_data <- read.csv("test_data.csv", header = T, sep = ",")
head(test_Income_data)
tail(test_Income_data)
```

# Review the structure of the training dataset
```{r}
str(train_Income_data)

```

# Review the structure of the test dataset
```{r}
str(test_Income_data)

```

# Review the summary of the training dataset
```{r}
summary(train_Income_data)

```

# Review the summary of the test dataset
```{r}
summary(test_Income_data)

```

# Subset the independent variables (X) from the training dataset to apply pre-processing
# The target/dependent variable is stored in y_train_Income_data
```{r}

X_train_Income_data <- train_Income_data[, -18]
head(X_train_Income_data)

y_train_Income_data <- train_Income_data[, 18]
head(y_train_Income_data)

```
# Apply KnnImpute method using Caret library to impute the missing values - training data

```{r}

sum(is.na(X_train_Income_data))

imputer_values <- preProcess(x = X_train_Income_data, method = "knnImpute")

X_train_Income_data <- predict(object = imputer_values, newdata = X_train_Income_data)

sum(is.na(X_train_Income_data))

summary(X_train_Income_data)


```

# Apply KnnImpute method using Caret library to impute the missing values - test data
# Imputation is done on the test data based on the training dataset to prevent the model from learning the test data

```{r}

sum(is.na(test_Income_data))

test_Income_data <- predict( object = imputer_values, newdata = test_Income_data)

sum(is.na(test_Income_data))

summary(test_Income_data)

```

# Since knnImpute from Caret ignores categorical attributes, we still see missing values
# we are applying centralImputation to impute missing values for the categorical attributes 

```{r}

X_train_Income_data <- centralImputation(X_train_Income_data) #Cenral Imputation

sum(is.na(X_train_Income_data))

summary(X_train_Income_data)

```

# Test Data - centralImputation to impute missing values for categorical attributes
```{r}

test_Income_data <- centralImputation(test_Income_data) #Cenral Imputation

sum(is.na(test_Income_data))

summary(test_Income_data)

```


##standardize data
```{r}

std_obj <- preProcess(x = X_train_Income_data,method = c("center", "scale"))

train_std_data <- predict(std_obj, X_train_Income_data)
head(train_std_data)
test_std_data <- predict(std_obj, test_Income_data)
head(test_std_data)

```


## Dummyify data

```{r}
dummy_obj <- dummyVars( ~ . , train_std_data)

train_dummy_data <- as.data.frame(predict(dummy_obj, train_std_data))
str(train_std_data)
test_dummy_data <- as.data.frame(predict(dummy_obj, test_std_data))
str(test_dummy_data)

```

## Check for colinearity on the numerical attributes

```{r}

num_attr <- X_train_Income_data[, c("age", "financial_weight", "years_of_education", "tax_paid", "loan_taken", "gain", "loss", "working_hours")]

str(num_attr)
cor(num_attr)

```

![Colinearity Matrix](Colinearity_Matrix.jpg)

#Above correlation matrix shows no need to apply VIF 

```{r}
X_train <- as.matrix(train_dummy_data)
y_train <- as.matrix(y_train_Income_data)

  
X_test <- as.matrix(test_dummy_data)


head(X_train)
head(y_train)

```

# Passing the matrix value to cv.glmnet - 
# Applying Regularization - Lasso (alpha = 1) to penalize the complexity of the model
```{r}

cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, type.measure = "mse", nfolds = 4)
print(cv_lasso)
plot(cv_lasso)

```
```{r}
str(cv_lasso)

plot(cv_lasso$glmnet.fit, xvar="lambda", label=TRUE)
```

```{r}

print(cv_lasso$lambda.min)

```
```{r}
coef(cv_lasso)


```


## Building the Final Model

```{r}
lasso_model <- glmnet(X_train, y_train, lambda = cv_lasso$lambda.min, alpha = 1)
str(lasso_model)
coef(lasso_model)

```

```{r}
lasso_model1 <- glmnet(X_train, y_train, lambda = cv_lasso$lambda.1se, alpha = 1)

coef(lasso_model1)


```
## Determine the model accuracy and the cut off value
```{r}

library(ROCR)
Acc_lasso <- predict(lasso_model1, X_train)
Acc_lasso <- prediction(Acc_lasso,y_train)

eval<-performance(Acc_lasso,"acc")# plotting a graph between aacuracy and prob values

plot(eval)
```

```{r}
max<-which.max(slot(eval,"y.values")[[1]])
acc<-slot(eval,"y.values")[[1]][max]#gives the point of max accuracy on the curve
print(acc)
c<-slot(eval,"x.values")[[1]][max] #gives the probability value corresponding to the max accuracy
print(c)
```
## Model with Lambda$min value - we have selected a Cut off value of 0.45
```{r}

preds_lasso <- predict(lasso_model, X_test)

preds_lasso <- ifelse(preds_lasso > 0.45, 1, 0)
print(preds_lasso)
```



```{r}

preds_lasso1 <- predict(lasso_model1, X_test)
preds_lasso1 <- ifelse(preds_lasso1 > 0.45, 1, 0)
print(preds_lasso1)
```

